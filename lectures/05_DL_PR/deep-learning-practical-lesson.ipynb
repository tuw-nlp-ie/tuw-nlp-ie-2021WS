{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/adaamko/0161526d638e1877f7b649b3fff8f3de/deep-learning-practical-lesson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUJ9cPl-Fry3"
   },
   "source": [
    "# Natural Language Processing and Information Extraction\n",
    "## Deep learning - practical session\n",
    "\n",
    "__Nov 12, 2021__\n",
    "\n",
    "__Ádám Kovács__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-08G4DavFry5"
   },
   "source": [
    "During this lecture we are going to use a classification dataset from a shared task: SemEval 2019 - Task 6. \n",
    "The dataset is about Identifying and Categorizing Offensive Language in Social Media.\n",
    "__Preparation:__\n",
    "- You will need the Semeval dataset (we will have code to download it)\n",
    "- You will need to install pytorch:\n",
    "    - pip install torch \n",
    "- You will also need to have pandas, torchtext, numpy and scikit learn installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_7oXPa0Fry6"
   },
   "source": [
    "We are going to use an open source library for building optimized deep learning models that can be run on GPUs, the library is called [Pytorch](https://pytorch.org/docs/stable/index.html). It is one of the most widely used libraries for building neural networks/deep learning models.\n",
    "\n",
    "In this lecture we are mostly using pure PyTorch models, but there are multiple libraries available to make it even easier to build neural networks. You are free to use them in your projects.\n",
    "Just to name a few:\n",
    "- TorchText: https://pytorch.org/text/stable/index.html\n",
    "- AllenNLP: https://github.com/allenai/allennlp\n",
    "\n",
    "__NOTE: It is advised to use Google Colab for this laboratory for free access to GPUs, and also for reproducibility.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZt9zm2LFry6",
    "outputId": "4e75b0b5-a7a2-4f59-fdf0-3b7f2238fd30"
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v4Xkf04_Fry7"
   },
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDFv7TnVFry8"
   },
   "source": [
    "## Download the dataset and load it into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0gNCsikFry8",
    "outputId": "5b322ec3-2674-4fec-9507-b271c51d9c7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/offenseval.tsv', <http.client.HTTPMessage at 0x7f1a58cfff50>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "u = urllib.request.URLopener()\n",
    "u.retrieve(\n",
    "    \"https://raw.githubusercontent.com/ZeyadZanaty/offenseval/master/datasets/training-v1/offenseval-training-v1.tsv\",\n",
    "    \"data/offenseval.tsv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xxkoCVUFry8"
   },
   "source": [
    "## Read in the dataset into a Pandas DataFrame\n",
    "Use `pd.read_csv` with the correct parameters to read in the dataset. If done correctly, `DataFrame` should have 5 columns, \n",
    "`id`, `tweet`, `subtask_a`, `subtask_b`, `subtask_c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kssSk4PbFry9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "14AzL6GHFry9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eef320fdacfdf485",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    train_data = pd.read_csv(\"./data/offenseval.tsv\", sep=\"\\t\")\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "rNCxGm0LFry-",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8f39b3b86623648c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a4337459-acbe-4602-8a3c-6ec8a79592f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>95338</td>\n",
       "      <td>@USER Sometimes I get strong vibes from people...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>67210</td>\n",
       "      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>82921</td>\n",
       "      <td>@USER And why report this garbage.  We don't g...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>27429</td>\n",
       "      <td>@USER Pussy</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>46552</td>\n",
       "      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0      86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2      16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3      62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "...      ...                                                ...       ...   \n",
       "13235  95338  @USER Sometimes I get strong vibes from people...       OFF   \n",
       "13236  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT   \n",
       "13237  82921  @USER And why report this garbage.  We don't g...       OFF   \n",
       "13238  27429                                        @USER Pussy       OFF   \n",
       "13239  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT   \n",
       "\n",
       "      subtask_b subtask_c  \n",
       "0           UNT       NaN  \n",
       "1           TIN       IND  \n",
       "2           NaN       NaN  \n",
       "3           UNT       NaN  \n",
       "4           NaN       NaN  \n",
       "...         ...       ...  \n",
       "13235       TIN       IND  \n",
       "13236       NaN       NaN  \n",
       "13237       TIN       OTH  \n",
       "13238       UNT       NaN  \n",
       "13239       NaN       NaN  \n",
       "\n",
       "[13240 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_unprocessed = read_dataset()\n",
    "\n",
    "train_data_unprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8sKz1zsFry-"
   },
   "source": [
    "## Convert `subtask_a` into a binary label\n",
    "The task is to classify the given tweets into two category: _offensive(OFF)_ , _not offensive (NOT)_. For machine learning algorithms you will need integer labels instead of strings. Add a new column to the dataframe called `label`, and transform the `subtask_a` column into a binary integer label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sQ4VazgwFry-",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-595b437c85da4194",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def transform(train_data):\n",
    "    labels = {\"NOT\": 0, \"OFF\": 1}\n",
    "\n",
    "    train_data[\"label\"] = [labels[item] for item in train_data.subtask_a]\n",
    "    train_data[\"tweet\"] = train_data[\"tweet\"].str.replace(\"@USER\", \"\")\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zSHYynQ3Fry_",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-56fa86b834804581",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_data = transform(train_data_unprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1vVLQwfFrzA"
   },
   "source": [
    "## Train a simple neural network on this dataset\n",
    "\n",
    "In this notebook we are going to build different neural architectures on the task:\n",
    "- A simple one layered feed forward neural network (FNN) with one-hot encoded vectors\n",
    "- Adding more layers to the FNN, making it a deep neural network\n",
    "- Instead of using one-hot encoded vectors we are going to add embedding vectors to the architecture, that takes the sequential nature of natural texts into account\n",
    "- Then we will train LSTM networks\n",
    "- At last, we will also build a Transformer architecture, that currently achieves SOTA results on a lot of tasks\n",
    "\n",
    "First we will build one-hot-encoded vectors for each sentence, and then use a simple feed forward neural network to predict the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kWIAidfzFrzA",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c242629b22cb523",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First we need to import pytorch and set a fixed random seed number for reproducibility\n",
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypaOR-1EFrzA"
   },
   "source": [
    "### Split the dataset into a train and a validation dataset\n",
    "Use the random seed for splitting. You should split the dataset into 70% training data and 30% validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LSeRGX4KFrzB",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20ba609174c640e7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "\n",
    "def split_data(train_data, random_seed):\n",
    "    tr_data, val_data = split(train_data, test_size=0.3, random_state=SEED)\n",
    "    return tr_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5bK6m0rRFrzB",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0e8a125310d3fea9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tr_data, val_data = split_data(train_data, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHxBtAYNFrzB"
   },
   "source": [
    "### Use CountVectorizer to prepare the features for the sentences\n",
    "_CountVectorizer_ is a great tool from _sklearn_ that helps us with basic preprocessing steps. It has lots of parameters to play with, you can check the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). It will:\n",
    "- Tokenize, lowercase the text\n",
    "- Filter out stopwords\n",
    "- Convert the text into one-hot encoded vectors\n",
    "- Select the _n_-best features\n",
    "\n",
    "We fit CountVectorizer using _3000_ features\n",
    "\n",
    "We will also _lemmatize_ texts using the _nltk_ package and its lemmatizer. Check the [docs](https://www.nltk.org/_modules/nltk/stem/wordnet.html) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shzT5AX0FrzC",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0943811065a971f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "dc5d747f-2c28-47fe-bf92-2fd3e6667a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/adaamko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/adaamko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "\n",
    "def prepare_vectorizer(tr_data):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=3000, tokenizer=LemmaTokenizer(), stop_words=\"english\"\n",
    "    )\n",
    "\n",
    "    word_to_ix = vectorizer.fit(tr_data.tweet)\n",
    "\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xq4NYTdcFrzC",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a2c6658aef3041dc",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "2bfc0491-9249-44b1-db40-215f0f46c23e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaamko/miniconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/adaamko/miniconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = prepare_vectorizer(tr_data)\n",
    "# The vocab size is the length of the vocabulary, or the length of the feature vectors\n",
    "VOCAB_SIZE = len(word_to_ix.vocabulary_)\n",
    "assert VOCAB_SIZE == 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer can directly transform any sentence into a one-hot encoded vector based on the corpus it was built upon.\n",
    "\n",
    "![onehot](https://miro.medium.com/max/886/1*_da_YknoUuryRheNS-SYWQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix.transform([\"Hello my name is adam\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "O4SD37O4FrzE",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d1819598c663eb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the correct device\n",
    "# It is important that every array should be on the same device or the training won't work\n",
    "# A device could be either the cpu or the gpu if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrlPM_d1FrzC"
   },
   "source": [
    "### Prepare the DataLoader for batch processing\n",
    "\n",
    "The __prepare_dataloader(..)__ function will take the training and the validation dataset and convert them to one-hot encoded vectors with the help of the initialized CountVectorizer.\n",
    "\n",
    "We prepare two FloatTensors and LongTensors for the converted tweets and labels of the training and the validation data.\n",
    "\n",
    "Then zip together the vectors with the labels as a list of tuples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "e-6VMlD-FrzD",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67b120b4ea6ba288",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Preparing the data loaders for the training and the validation sets\n",
    "# PyTorch operates on it's own datatype which is very similar to numpy's arrays\n",
    "# They are called Torch Tensors: https://pytorch.org/docs/stable/tensors.html\n",
    "# They are optimized for training neural networks\n",
    "def prepare_dataloader(tr_data, val_data, word_to_ix):\n",
    "    # First we transform the tweets into one-hot encoded vectors\n",
    "    # Then we create Torch Tensors from the list of the vectors\n",
    "    # It is also inportant to send the Tensors to the correct device\n",
    "    # All of the tensors should be on the same device when training\n",
    "    tr_data_vecs = torch.FloatTensor(word_to_ix.transform(tr_data.tweet).toarray()).to(\n",
    "        device\n",
    "    )\n",
    "    tr_labels = torch.LongTensor(tr_data.label.tolist()).to(device)\n",
    "\n",
    "    val_data_vecs = torch.FloatTensor(\n",
    "        word_to_ix.transform(val_data.tweet).toarray()\n",
    "    ).to(device)\n",
    "    val_labels = torch.LongTensor(val_data.label.tolist()).to(device)\n",
    "\n",
    "    tr_data_loader = [(sample, label) for sample, label in zip(tr_data_vecs, tr_labels)]\n",
    "    val_data_loader = [\n",
    "        (sample, label) for sample, label in zip(val_data_vecs, val_labels)\n",
    "    ]\n",
    "\n",
    "    return tr_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NQG5rdlpFrzD",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-212fb18e207761c4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tr_data_loader, val_data_loader = prepare_dataloader(tr_data, val_data, word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxHwnckHFrzD"
   },
   "source": [
    "- __We have the correct lists now, it is time to initialize the DataLoader objects!__\n",
    "- __Create two DataLoader objects with the lists we have created__\n",
    "- __Shuffle the training data but not the validation data!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "i1DPygZyFrzE"
   },
   "outputs": [],
   "source": [
    "# We then define a BATCH_SIZE for our model\n",
    "# Usually we don't feed the whole dataset into our model at once\n",
    "# For this we have the BATCH_SIZE parameter\n",
    "# Try to experiment with different sized batches and see if changing this will improve the performance or not!\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BT3hbbGjFrzD",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96ac025a45bc4fec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# The DataLoader(https://pytorch.org/docs/stable/data.html) class helps us to prepare the training batches\n",
    "# It has a lot of useful parameters, one of it is _shuffle_ which will randomize the training dataset in each epoch\n",
    "# This can also improve the performance of our model\n",
    "def create_dataloader_iterators(tr_data_loader, val_data_loader, BATCH_SIZE):\n",
    "    train_iterator = DataLoader(\n",
    "        tr_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    valid_iterator = DataLoader(\n",
    "        val_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_iterator, valid_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Zvvqcuk3FrzE",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7b88321ec3ee1096",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator = create_dataloader_iterators(\n",
    "    tr_data_loader, val_data_loader, BATCH_SIZE\n",
    ")\n",
    "assert type(train_iterator) == torch.utils.data.dataloader.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WUTDwv2FrzE"
   },
   "source": [
    "### Building the first PyTorch model\n",
    "At first, the model will contain a single Linear layer that takes one-hot-encoded vectors and trainsforms it into the dimension of the __NUM_LABELS__(how many classes we are trying to predict). Then, run through the output on a softmax activation to produce probabilites of the classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HqwUwPUdFrzF",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00fb572132edf99a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Note that we could add more Linear Layers here connected to each other\n",
    "        # Then we would also need to have a HIDDEN_SIZE hyperparameter as an input to our model\n",
    "        # Then, with activation functions between them (e.g. RELU) we could have a \"Deep\" model\n",
    "        # This is just an example for a shallow network\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec, sequence_lens):\n",
    "        # Ignore sequence_lens for now!\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        # Softmax will provide a probability distribution among the classes\n",
    "        # We can then use this for our loss function\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Lr-4sjO3FrzF",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3cbec9b993598632",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The INPUT_DIM is the size of our input vectors\n",
    "INPUT_DIM = VOCAB_SIZE\n",
    "# We have only 2 classes\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vVxRlyR-FrzF",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-203697b21306ccb9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Init the model\n",
    "# At first it is untrained, the weights are assigned random\n",
    "model = BoWClassifier(OUTPUT_DIM, INPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "UEIXHRUTFrzF"
   },
   "outputs": [],
   "source": [
    "# Set the optimizer and the loss function!\n",
    "# https://pytorch.org/docs/stable/optim.html\n",
    "import torch.optim as optim\n",
    "\n",
    "# The optimizer will update the weights of our model based on the loss function\n",
    "# This is essential for correct training\n",
    "# The _lr_ parameter is the learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "K0pDsTdKFrzG",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9852177c09074615",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Copy the model and the loss function to the correct device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_BSC-ttlFrzG",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-50c925cbe0576fd3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert model.linear.out_features == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCrTcch_FrzG"
   },
   "source": [
    "### Training and evaluating PyTorch models\n",
    "- __calculate_performance__: This should calculate the batch-wise precision, recall, and fscore of your model!\n",
    "- __train__ - Train your model on the training data! This function should set the model to training mode, then use the given iterator to iterate through the training samples and make predictions using the provided model. You should then propagate back the error with the loss function and the optimizer. Finally return the average epoch loss and performance!\n",
    "- __evaluate__ - Evaluate your model on the validation dataset. This function is essentially the same as the trainnig function, but you should set your model to eval mode and don't propagate back the errors to your weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9TWQekCdFrzG",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1818f7b4bce37196",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def calculate_performance(preds, y):\n",
    "    \"\"\"\n",
    "    Returns precision, recall, fscore per batch\n",
    "    \"\"\"\n",
    "    # Get the predicted label from the probabilities\n",
    "    rounded_preds = preds.argmax(1)\n",
    "\n",
    "    # Calculate the correct predictions batch-wise and calculate precision, recall, and fscore\n",
    "    # WARNING: Tensors here could be on the GPU, so make sure to copy everything to CPU\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        rounded_preds.cpu(), y.cpu()\n",
    "    )\n",
    "\n",
    "    return precision[1], recall[1], fscore[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rhzACM5IFrzG",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea8beb3df906f9a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    # We will calculate loss and accuracy epoch-wise based on average batch accuracy\n",
    "    epoch_loss = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_fscore = 0\n",
    "\n",
    "    # You always need to set your model to training mode\n",
    "    # If you don't set your model to training mode the error won't propagate back to the weights\n",
    "    model.train()\n",
    "\n",
    "    # We calculate the error on batches so the iterator will return matrices with shape [BATCH_SIZE, VOCAB_SIZE]\n",
    "    for batch in iterator:\n",
    "        text_vecs = batch[0]\n",
    "        labels = batch[1]\n",
    "        sen_lens = []\n",
    "        texts = []\n",
    "\n",
    "        # This is for later!\n",
    "        if len(batch) > 2:\n",
    "            sen_lens = batch[2]\n",
    "            texts = batch[3]\n",
    "\n",
    "        # We reset the gradients from the last step, so the loss will be calculated correctly (and not added together)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # This runs the forward function on your model (you don't need to call it directly)\n",
    "        predictions = model(text_vecs, sen_lens)\n",
    "\n",
    "        # Calculate the loss and the accuracy on the predictions (the predictions are log probabilities, remember!)\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "\n",
    "        # Propagate the error back on the model (this means changing the initial weights in your model)\n",
    "        # Calculate gradients on parameters that requries grad\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # We add batch-wise loss to the epoch-wise loss\n",
    "        epoch_loss += loss.item()\n",
    "        # We also do the same with the scores\n",
    "        epoch_prec += prec.item()\n",
    "        epoch_recall += recall.item()\n",
    "        epoch_fscore += fscore.item()\n",
    "    return (\n",
    "        epoch_loss / len(iterator),\n",
    "        epoch_prec / len(iterator),\n",
    "        epoch_recall / len(iterator),\n",
    "        epoch_fscore / len(iterator),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ASFglaVtFrzH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-810fadb1db8e2028",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The evaluation is done on the validation dataset\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_fscore = 0\n",
    "    # On the validation dataset we don't want training so we need to set the model on evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Also tell Pytorch to not propagate any error backwards in the model or calculate gradients\n",
    "    # This is needed when you only want to make predictions and use your model in inference mode!\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # The remaining part is the same with the difference of not using the optimizer to backpropagation\n",
    "        for batch in iterator:\n",
    "            text_vecs = batch[0]\n",
    "            labels = batch[1]\n",
    "            sen_lens = []\n",
    "            texts = []\n",
    "\n",
    "            if len(batch) > 2:\n",
    "                sen_lens = batch[2]\n",
    "                texts = batch[3]\n",
    "\n",
    "            predictions = model(text_vecs, sen_lens)\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_prec += prec.item()\n",
    "            epoch_recall += recall.item()\n",
    "            epoch_fscore += fscore.item()\n",
    "\n",
    "    # Return averaged loss on the whole epoch!\n",
    "    return (\n",
    "        epoch_loss / len(iterator),\n",
    "        epoch_prec / len(iterator),\n",
    "        epoch_recall / len(iterator),\n",
    "        epoch_fscore / len(iterator),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "PzRshE3-FrzH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73c8635f8fc4a7fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# This is just for measuring training time!\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBcx1vxBFrzH"
   },
   "source": [
    "### Training loop!\n",
    "Below is the training loop of our model! Try to set an EPOCH number that will correctly train your model :) (it is not underfitted but neither overfitted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "sGaJR3xTFrzH"
   },
   "outputs": [],
   "source": [
    "def training_loop(epoch_number=15):\n",
    "    # Set an EPOCH number!\n",
    "    N_EPOCHS = epoch_number\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "\n",
    "    # We loop forward on the epoch number\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model on the training set using the dataloader\n",
    "        train_loss, train_prec, train_rec, train_fscore = train(\n",
    "            model, train_iterator, optimizer, criterion\n",
    "        )\n",
    "        # And validate your model on the validation set\n",
    "        valid_loss, valid_prec, valid_rec, valid_fscore = evaluate(\n",
    "            model, valid_iterator, criterion\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # If we find a better model, we save the weights so later we may want to reload it\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"tut1-model.pt\")\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(\n",
    "            f\"\\tTrain Loss: {train_loss:.3f} | Train Prec: {train_prec*100:.2f}% | Train Rec: {train_rec*100:.2f}% | Train Fscore: {train_fscore*100:.2f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\t Val. Loss: {valid_loss:.3f} |  Val Prec: {valid_prec*100:.2f}% | Val Rec: {valid_rec*100:.2f}% | Val Fscore: {valid_fscore*100:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIm70jWgQq21",
    "outputId": "e88508b0-dd40-48b7-9d56-0b9066d23eca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaamko/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.650 | Train Prec: 4.12% | Train Rec: 36.46% | Train Fscore: 6.40%\n",
      "\t Val. Loss: 0.628 |  Val Prec: 3.95% | Val Rec: 57.54% | Val Fscore: 7.26%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.599 | Train Prec: 9.34% | Train Rec: 81.77% | Train Fscore: 16.34%\n",
      "\t Val. Loss: 0.604 |  Val Prec: 11.87% | Val Rec: 82.45% | Val Fscore: 20.29%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.566 | Train Prec: 19.76% | Train Rec: 92.99% | Train Fscore: 31.78%\n",
      "\t Val. Loss: 0.586 |  Val Prec: 16.35% | Val Rec: 82.04% | Val Fscore: 26.71%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train Prec: 27.47% | Train Rec: 91.10% | Train Fscore: 41.35%\n",
      "\t Val. Loss: 0.574 |  Val Prec: 21.29% | Val Rec: 77.86% | Val Fscore: 32.89%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.518 | Train Prec: 33.10% | Train Rec: 90.33% | Train Fscore: 47.47%\n",
      "\t Val. Loss: 0.564 |  Val Prec: 25.48% | Val Rec: 78.34% | Val Fscore: 37.48%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.501 | Train Prec: 37.35% | Train Rec: 90.05% | Train Fscore: 51.96%\n",
      "\t Val. Loss: 0.557 |  Val Prec: 28.56% | Val Rec: 76.84% | Val Fscore: 40.66%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.486 | Train Prec: 40.65% | Train Rec: 89.60% | Train Fscore: 55.38%\n",
      "\t Val. Loss: 0.551 |  Val Prec: 31.25% | Val Rec: 76.88% | Val Fscore: 43.51%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.473 | Train Prec: 43.17% | Train Rec: 89.95% | Train Fscore: 57.48%\n",
      "\t Val. Loss: 0.547 |  Val Prec: 34.55% | Val Rec: 75.52% | Val Fscore: 46.51%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.461 | Train Prec: 45.85% | Train Rec: 88.27% | Train Fscore: 59.60%\n",
      "\t Val. Loss: 0.544 |  Val Prec: 36.83% | Val Rec: 74.93% | Val Fscore: 48.53%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train Prec: 48.85% | Train Rec: 88.16% | Train Fscore: 62.08%\n",
      "\t Val. Loss: 0.541 |  Val Prec: 36.75% | Val Rec: 75.56% | Val Fscore: 48.64%\n",
      "Epoch: 11 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.442 | Train Prec: 49.57% | Train Rec: 89.16% | Train Fscore: 63.00%\n",
      "\t Val. Loss: 0.539 |  Val Prec: 39.62% | Val Rec: 73.43% | Val Fscore: 50.67%\n",
      "Epoch: 12 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train Prec: 51.84% | Train Rec: 87.89% | Train Fscore: 64.61%\n",
      "\t Val. Loss: 0.537 |  Val Prec: 39.06% | Val Rec: 74.80% | Val Fscore: 50.49%\n",
      "Epoch: 13 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train Prec: 52.44% | Train Rec: 88.85% | Train Fscore: 65.30%\n",
      "\t Val. Loss: 0.537 |  Val Prec: 40.93% | Val Rec: 73.13% | Val Fscore: 51.69%\n",
      "Epoch: 14 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train Prec: 54.66% | Train Rec: 88.31% | Train Fscore: 66.93%\n",
      "\t Val. Loss: 0.536 |  Val Prec: 41.13% | Val Rec: 73.25% | Val Fscore: 51.90%\n",
      "Epoch: 15 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.412 | Train Prec: 55.17% | Train Rec: 88.59% | Train Fscore: 67.19%\n",
      "\t Val. Loss: 0.536 |  Val Prec: 43.18% | Val Rec: 72.70% | Val Fscore: 53.38%\n"
     ]
    }
   ],
   "source": [
    "training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4B_wdUgFrzI"
   },
   "source": [
    "\n",
    "__NOTE: DON'T FORGET TO RERUN THE MODEL INITIALIZATION WHEN YOU ARE TRYING TO RUN THE MODEL MULTIPLE TIMES. IF YOU DON'T REINITIALIZE THE MODEL IT WILL CONTINUE THE TRAINING WHERE IT HAS STOPPED LAST TIME AND DOESN'T RUN FROM SRATCH!__\n",
    "\n",
    "These lines:\n",
    "\n",
    "```python\n",
    "model = BoWClassifier(OUTPUT_DIM, INPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "```\n",
    "\n",
    "This will reinitialize the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "eOpuzSyfPwBh"
   },
   "outputs": [],
   "source": [
    "def reinitialize(model):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.NLLLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "vgflJA47P3O-"
   },
   "outputs": [],
   "source": [
    "reinitialize(BoWClassifier(OUTPUT_DIM, INPUT_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFcqasArFrzI"
   },
   "source": [
    "## Add more linear layers to the model and experiment with other hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSv9C_heFrzJ"
   },
   "source": [
    "### More layers\n",
    "\n",
    "Currently we only have a single linear layers in our model. We are now adding more linear layers to the model.\n",
    "We also introduce a HIDDEN_SIZE parameter that will be the size of the intermediate representation between the linear layers. Also adding a RELU activation function between the linear layers.\n",
    "\n",
    "See more:\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "- https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "_S4Ytz8HFrzJ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f71eea2d6e70ad97",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BoWDeepClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, hidden_size):\n",
    "        super(BoWDeepClassifier, self).__init__()\n",
    "        # First linear layer\n",
    "        self.linear1 = nn.Linear(vocab_size, hidden_size)\n",
    "        # Non-linear activation function between them\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # Second layer\n",
    "        self.linear2 = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec, sequence_lens):\n",
    "        # Run the input vector through every layer\n",
    "        output = self.linear1(bow_vec)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear2(output)\n",
    "\n",
    "        # Get the probabilities\n",
    "        return F.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "A5tw97UGFrzK"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 200\n",
    "learning_rate = 0.001\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "7flTZ3vgFrzK"
   },
   "outputs": [],
   "source": [
    "model = BoWDeepClassifier(OUTPUT_DIM, INPUT_DIM, HIDDEN_SIZE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNZV21ShFrzK",
    "outputId": "4bc2112f-1c98-47c7-f8cd-a58bbc662df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.596 | Train Prec: 20.60% | Train Rec: 49.89% | Train Fscore: 25.92%\n",
      "\t Val. Loss: 0.539 |  Val Prec: 36.23% | Val Rec: 74.23% | Val Fscore: 47.90%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.429 | Train Prec: 57.83% | Train Rec: 79.56% | Train Fscore: 66.12%\n",
      "\t Val. Loss: 0.552 |  Val Prec: 48.22% | Val Rec: 69.31% | Val Fscore: 55.94%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train Prec: 72.06% | Train Rec: 85.52% | Train Fscore: 77.73%\n",
      "\t Val. Loss: 0.597 |  Val Prec: 52.68% | Val Rec: 64.65% | Val Fscore: 57.25%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train Prec: 82.02% | Train Rec: 90.61% | Train Fscore: 85.80%\n",
      "\t Val. Loss: 0.673 |  Val Prec: 51.16% | Val Rec: 63.78% | Val Fscore: 56.00%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train Prec: 87.18% | Train Rec: 94.32% | Train Fscore: 90.37%\n",
      "\t Val. Loss: 0.775 |  Val Prec: 52.20% | Val Rec: 61.79% | Val Fscore: 55.82%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train Prec: 91.57% | Train Rec: 96.76% | Train Fscore: 93.94%\n",
      "\t Val. Loss: 0.845 |  Val Prec: 50.86% | Val Rec: 62.27% | Val Fscore: 55.24%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train Prec: 94.51% | Train Rec: 97.59% | Train Fscore: 95.95%\n",
      "\t Val. Loss: 0.914 |  Val Prec: 53.92% | Val Rec: 60.90% | Val Fscore: 56.52%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train Prec: 95.54% | Train Rec: 98.24% | Train Fscore: 96.79%\n",
      "\t Val. Loss: 0.987 |  Val Prec: 51.19% | Val Rec: 61.37% | Val Fscore: 55.11%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train Prec: 96.63% | Train Rec: 98.49% | Train Fscore: 97.49%\n",
      "\t Val. Loss: 1.062 |  Val Prec: 51.25% | Val Rec: 60.86% | Val Fscore: 55.05%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train Prec: 96.81% | Train Rec: 98.79% | Train Fscore: 97.75%\n",
      "\t Val. Loss: 1.114 |  Val Prec: 50.66% | Val Rec: 60.60% | Val Fscore: 54.60%\n",
      "Epoch: 11 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.047 | Train Prec: 97.28% | Train Rec: 98.90% | Train Fscore: 98.04%\n",
      "\t Val. Loss: 1.164 |  Val Prec: 52.08% | Val Rec: 59.77% | Val Fscore: 54.99%\n",
      "Epoch: 12 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.042 | Train Prec: 97.49% | Train Rec: 99.28% | Train Fscore: 98.33%\n",
      "\t Val. Loss: 1.218 |  Val Prec: 53.23% | Val Rec: 57.02% | Val Fscore: 54.24%\n",
      "Epoch: 13 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.039 | Train Prec: 97.53% | Train Rec: 99.06% | Train Fscore: 98.24%\n",
      "\t Val. Loss: 1.283 |  Val Prec: 51.05% | Val Rec: 58.45% | Val Fscore: 53.83%\n",
      "Epoch: 14 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.036 | Train Prec: 98.01% | Train Rec: 99.04% | Train Fscore: 98.48%\n",
      "\t Val. Loss: 1.342 |  Val Prec: 50.69% | Val Rec: 60.09% | Val Fscore: 54.37%\n",
      "Epoch: 15 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.032 | Train Prec: 97.71% | Train Rec: 99.54% | Train Fscore: 98.58%\n",
      "\t Val. Loss: 1.351 |  Val Prec: 53.43% | Val Rec: 57.19% | Val Fscore: 54.64%\n"
     ]
    }
   ],
   "source": [
    "training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRInQ1ToFrzK"
   },
   "source": [
    "## Implement automatic early-stopping in the training loop\n",
    "Early stopping is a very easy method to avoid the overfitting of your model.\n",
    "\n",
    "We could:\n",
    "- Save the training and the validation loss of the last two epochs (if you are atleast in the third epoch)\n",
    "- If the loss increased in the last two epoch on the training data but descreased or stagnated in the validation data, you should stop the training automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pblBEo87FrzL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d460cc4aa3dd437",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# REINITIALIZE YOUR MODEL TO GET A CORRECT RUN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAN4Y5s3FrzL"
   },
   "source": [
    "## Handling class imbalance\n",
    "Our data is imbalanced, the first class has twice the population of the second class.\n",
    "\n",
    "One way of handling imbalanced data is to weight the loss function, so it penalizes errors on the smaller class.\n",
    "\n",
    "Look at the documentation of the loss function: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "\n",
    "Set the weights based on the inverse population of the classes (so the less sample a class has, more the errors will be penalized!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9_wXYqlFrzL",
    "outputId": "f5caf253-e85b-4e70-f9c8-b4434a4b0e3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    6179\n",
       "1    3089\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.groupby(\"label\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "5FoqDTQIFrzL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6ebf131781a3332d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "weights = torch.Tensor([1, 2])\n",
    "criterion = nn.NLLLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBu230DWRYwK"
   },
   "source": [
    "## Adding an Embedding Layer to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We only used one-hot-encoded vectors as our features until now\n",
    "- Now we will introduce an [embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer into our network.\n",
    "- We will feed the words into our network one-by-one, and the layer will learn a dense vector representation for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![embeddingbag](https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png)\n",
    "\n",
    "_from pytorch.org_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "r5uYW1OmUpfJ"
   },
   "outputs": [],
   "source": [
    "# Get the analyzer to get the word-id mapping from CountVectorizer\n",
    "an = word_to_ix.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Syg7NPVxUsWZ",
    "outputId": "e29a5029-6cef-4699-b403-ba237a0fbd84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'adam']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an(\"hello my name is adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jRJ_cbeeUvMy",
    "outputId": "5e6ea91c-52d9-47eb-b609-948b0a5d934f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🤨'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(word_to_ix.vocabulary_, key=word_to_ix.vocabulary_.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DG9xZYBQ7VNQ",
    "outputId": "1956ab04-bd4f-426f-e053-4c663bacfec4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_ix.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "dbU3fq40UxyR"
   },
   "outputs": [],
   "source": [
    "def create_input(dataset, analyzer, vocabulary):\n",
    "    dataset_as_indices = []\n",
    "\n",
    "    # We go through each tweet in the dataset\n",
    "    # We need to add two additional symbols to the vocabulary\n",
    "    # We have 3000 features, ranged 0-2999\n",
    "    # We add 3000 as an id for the \"unknown\" words not among the features\n",
    "    # 3001 will be the symbol for padding, but about this later!\n",
    "    for tweet in dataset:\n",
    "        tokens = analyzer(tweet)\n",
    "        token_ids = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # if the token is in the vocab, we add the id\n",
    "            if token in vocabulary:\n",
    "                token_ids.append(vocabulary[token])\n",
    "            # else we add the id of the unknown token\n",
    "            else:\n",
    "                token_ids.append(3000)\n",
    "\n",
    "        # if we removed every token during preprocessing (stopword removal, lemmatization), we add the unknown token to the list so it won't be empty\n",
    "        if not token_ids:\n",
    "            token_ids.append(3000)\n",
    "        dataset_as_indices.append(torch.LongTensor(token_ids).to(device))\n",
    "\n",
    "    return dataset_as_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "xDZmvZmwU3oI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaamko/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/adaamko/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# We add the length of the tweets so sentences with similar lengths will be next to each other\n",
    "# This can be important because of padding\n",
    "tr_data[\"length\"] = tr_data.tweet.str.len()\n",
    "val_data[\"length\"] = val_data.tweet.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3SypHXUVAOR",
    "outputId": "83d0630a-da0f-45cc-adc3-68b61d812ceb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1224      59\n",
       "9102      14\n",
       "3655      28\n",
       "8201      92\n",
       "6141      62\n",
       "        ... \n",
       "11468     63\n",
       "7221     275\n",
       "1318      52\n",
       "8915      50\n",
       "11055    167\n",
       "Name: tweet, Length: 9268, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.tweet.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "tJyVrIpLVHjA"
   },
   "outputs": [],
   "source": [
    "tr_data = tr_data.sort_values(by=\"length\")\n",
    "val_data = val_data.sort_values(by=\"length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "aLxMeEF6VJto"
   },
   "outputs": [],
   "source": [
    "# We create the dataset as ids of tokens\n",
    "dataset_as_ids = create_input(tr_data.tweet, an, word_to_ix.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXRm0Qc0VMRo",
    "outputId": "cd490df2-1a6c-4410-96dc-c5a6bffff656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2366], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_as_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "- We didn't need to take care of input padding when using one-hot-encoded vectors\n",
    "- Padding handles different sized inputs\n",
    "- We can pad sequences from the left, or from the right\n",
    "\n",
    "![padding](https://miro.medium.com/max/1218/1*zsIXWoN0_CE9PXzmY3tIjQ.png)\n",
    "\n",
    "_image from https://towardsdatascience.com/nlp-preparing-text-for-deep-learning-model-using-tensorflow2-461428138657_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "wlFl5MTtVb8J"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# pad_sequence will take care of the padding\n",
    "# we will need to provide a padding_value to it\n",
    "padded = pad_sequence(dataset_as_ids, batch_first=True, padding_value=3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "RCPFCrMbXIAs"
   },
   "outputs": [],
   "source": [
    "def prepare_dataloader_with_padding(tr_data, val_data, word_to_ix):\n",
    "    # First create the id representations of the input vectors\n",
    "    # Then pad the sequences so all of the input is the same size\n",
    "    # We padded texts for the whole dataset, this could have been done batch-wise also!\n",
    "    tr_data_vecs = pad_sequence(\n",
    "        create_input(tr_data.tweet, an, word_to_ix.vocabulary_),\n",
    "        batch_first=True,\n",
    "        padding_value=3001,\n",
    "    )\n",
    "    tr_labels = torch.LongTensor(tr_data.label.tolist()).to(device)\n",
    "    tr_lens = torch.LongTensor(\n",
    "        [len(i) for i in create_input(tr_data.tweet, an, word_to_ix.vocabulary_)]\n",
    "    )\n",
    "\n",
    "    # We also add the texts to the batches\n",
    "    # This is for the Transformer models, you wont need this in the next experiments\n",
    "    tr_sents = tr_data.tweet.tolist()\n",
    "\n",
    "    val_data_vecs = pad_sequence(\n",
    "        create_input(val_data.tweet, an, word_to_ix.vocabulary_),\n",
    "        batch_first=True,\n",
    "        padding_value=3001,\n",
    "    )\n",
    "    val_labels = torch.LongTensor(val_data.label.tolist()).to(device)\n",
    "    val_lens = torch.LongTensor(\n",
    "        [len(i) for i in create_input(val_data.tweet, an, word_to_ix.vocabulary_)]\n",
    "    )\n",
    "\n",
    "    val_sents = val_data.tweet.tolist()\n",
    "\n",
    "    tr_data_loader = [\n",
    "        (sample, label, length, sent)\n",
    "        for sample, label, length, sent in zip(\n",
    "            tr_data_vecs, tr_labels, tr_lens, tr_sents\n",
    "        )\n",
    "    ]\n",
    "    val_data_loader = [\n",
    "        (sample, label, length, sent)\n",
    "        for sample, label, length, sent in zip(\n",
    "            val_data_vecs, val_labels, val_lens, val_sents\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return tr_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "EZvbpqCwXY3E"
   },
   "outputs": [],
   "source": [
    "tr_data_loader, val_data_loader = prepare_dataloader_with_padding(\n",
    "    tr_data, val_data, word_to_ix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "lBSJgL9rXgDG"
   },
   "outputs": [],
   "source": [
    "def create_dataloader_iterators_with_padding(\n",
    "    tr_data_loader, val_data_loader, BATCH_SIZE\n",
    "):\n",
    "    train_iterator = DataLoader(\n",
    "        tr_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    valid_iterator = DataLoader(\n",
    "        val_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_iterator, valid_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "_XLM8WHqZCim"
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator = create_dataloader_iterators_with_padding(\n",
    "    tr_data_loader, val_data_loader, BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrBaXBnaWk9t",
    "outputId": "f0ab7a27-1690-4893-b2c2-62695c6776ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 581,   21, 2786,  ..., 3001, 3001, 3001],\n",
       "         [  11, 1457, 2293,  ..., 3001, 3001, 3001],\n",
       "         [ 721,  208, 3000,  ..., 3001, 3001, 3001],\n",
       "         ...,\n",
       "         [   1, 1635, 3000,  ..., 3001, 3001, 3001],\n",
       "         [2380, 3000,   21,  ..., 3001, 3001, 3001],\n",
       "         [1209, 2980, 2741,  ..., 3001, 3001, 3001]], device='cuda:0'),\n",
       " tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1], device='cuda:0'),\n",
       " tensor([ 3,  6, 14, 13, 13, 24, 29, 17, 17, 14, 30,  9, 14,  2, 31,  2, 19,  3,\n",
       "         25,  2, 10,  9,  8,  5,  3, 10,  7, 27,  8, 25, 22,  2, 30, 31, 14, 20,\n",
       "         12,  2,  9, 25, 16, 18, 17, 12, 23,  7,  7, 20, 10,  4, 18, 11,  9,  8,\n",
       "          7,  2, 32, 27,  2, 24, 16,  4,  8, 14]),\n",
       " ('      They coming. URL',\n",
       "  \"  That's just what Satan would say if he were threatened.\",\n",
       "  'And yet I see daily Antifa doxing without any suspensions whatsoever. Not to mention CNN doxxing and threatening a meme creator. URL',\n",
       "  \" It has been being de-created slowly and stealthily since the CONservatives came to power... Hunt's done a sterling job.\",\n",
       "  '#parklandkids misappropriated the phrase #neveragain in order to do exactly the opposite of what it originally stood for.  URL',\n",
       "  \" Democrats haven't done anything but cover up ..so all you think you have a solution .ya don't.....you want gun control.? How about abortion control. Free medical. Free ed. Quit tripping\",\n",
       "  '  #DUMMYDONNIEJR 🤦🏻\\u200d♀️🤥🤡🤷🏻\\u200d♀️  THIS IS FROM 2008 IDIOT 🤣😂😅.  The apple doesn’t fall to far from the tree when it comes to lying 🤥.   The American people have had #EnoughIsEnough of #LIARINCHIEF and you are actually an embarrassment as well like your daddy 🍄🍄',\n",
       "  '     just need to hang in strong together such actions will hand tories a life line on a plate ? without  are going to be ripped apart come the next general election',\n",
       "  \"  Xtc isn't going to help you sleep hahahah but it will help you feel better after how Rockstar is shitting on us. Or did you mean sleeping pills😂\",\n",
       "  ' #CrookedHillary are you drunk again?  Go back to bed granny.#TrumpTrain #MAGA #Deplorables #DregOfSociety',\n",
       "  '10)  But why?  Do they hate us that much?\"    Well yes and no.  For the Leftists, yes they do.  Why?  Because they\\'re Leftists, silly!  You either get with their program and let them tell you what to do - or they hate you.  With the formerly conservative conservatives - no.\"',\n",
       "  '  Wise move .. the only things the Democrats are doing is creating anger regarding conservatives',\n",
       "  'Replace Chris Collins with anyone but Carl Paladino\"  URL #TCOT #MAGA #RedNationRising\"',\n",
       "  '           Nah they are liberals',\n",
       "  \"   Oh...July...so like 35 &amp; 3/4 years ago...  NO...that's a difference maker.  I had no idea!   Please share your thoughts on serial woman abuser &amp; Antifa lover  🤔\",\n",
       "  '    Besides the Liberals.',\n",
       "  ' aw. *hugs* i’ve gotten flustered like that as well. it sucks. i also sometimes write myself a little script. you’re not alone.',\n",
       "  '  They probably get bonus pay for that',\n",
       "  ' Get him some line help. He is gonna be just fine. As the game went on you could see him progressing more with his reads. He brought what has been missing. The deep ball presence. Now he just needs a little more time',\n",
       "  ' Sure thing😏',\n",
       "  \"    Or in other words. ignore a democratic vote as it didn't go the way I wanted.\",\n",
       "  \" I'll be looking for you on here election night...make sure you are on Twitter.\",\n",
       "  ' Holy fuck I’m so sorry someone violated your trust like that',\n",
       "  ' IM VERY HORIBBLE AND SHITTY THATS HOW I AM😭',\n",
       "  '   i have heard he is making waves',\n",
       "  '  coming from a guy who should be arrested and charged as an accessory. I hat a deranged individual.',\n",
       "  '      Conservatives who love draft dodgers. I love it.',\n",
       "  ' This is all political theater. Are liberals really that stupid to believe that the FBI investigates state crimes? Especially incidents that happened 36 years ago by a juvenile? This is nothing more than a tactic to delay Kavanaugh’s confirmation.',\n",
       "  '   #AntiFA  had their sites on Collins for awhile.   #TooSoon',\n",
       "  '  You don’t understand who he is and is capablities. Despite the proof in the economy.  My hypothesis is that you have been lied to by liberals your entire life and are believing what you are told. The liberal media made bill Clinton look good and could do the same with trump.',\n",
       "  ' If it doesn’t have m&amp;m’s it’s not trail mix. You are kicking ass and doing awesome. Keep it up!!!',\n",
       "  ' He is! 💥',\n",
       "  ' Not really. Gun control limits one capability to personally safety. It makes it even harder for good people to buy guns legally. It will even make it hard for women to obtain guns for self defense. Just keep a simple background check system we already have.',\n",
       "  '    I\\'m actually having some sympathy for Chris\\' point here. It seems the line\" has moved so far to the left that actual liberals have more in common with conservatives than what passes for the left these days. (\\'cause whatever the left is, it ain\\'t liberal)\"',\n",
       "  '  She is an hypocrite and back stabber like her boss! She applaud this clip. Neighbors help each other when they need help?  URL',\n",
       "  '  American terrorists. We need strict punishment for these lawless thugs. STOP powdering Antifa’s ass.  #BackTheBlue 💙🖤 #gettough',\n",
       "  '  The Gibraltar people will be hostages in the remaining #Brexit negotiations. Do  or the  care? Not a fig!',\n",
       "  '        He is a sexual being 😋',\n",
       "  ' The set-up that led to the second Hogan touchdown shows how much of a threat he is and how much defenses respect where he is on the field',\n",
       "  ' Yes Comie (communist pun intended) WE do and will emerge stronger when u &amp; all ur cronies are put to death at Gitmo. WE THE PEOPLE WILL INDEED BE STRONGER. #MAGA #DrainTheSwamp',\n",
       "  '  you have a lot to atone for. You better atone for all your lies. I used to think you were a better person. I can’t see anything but evil.',\n",
       "  '     Flake shld end his search for relevancy\". He\\'s RINO filled w hatred for POTUS. #Despicable behavior\"',\n",
       "  'Active shooter Middleton WI several ambulances 🚑 on stand by waiting for police to clear building #MAGA #TrumpTrain   #KeepAmericaGreat2020 ',\n",
       "  \"   Why can't liberals read??????  READ it AGAIN URL\",\n",
       "  'TRYING TO HIDE ALL THE EVIDENCE 😂😎✍👀 #MAGA #QANON #TRUMP #WWG1WGA #QARMY #QALERT #MAGAFORALLINC  🤔 #TOOLATE😎 👇 URL',\n",
       "  ' Yay! You are unblocked 😂 I see how you gently responded.',\n",
       "  ' i love you honey i hope you are okay take your time and take care of yourself 💕',\n",
       "  '    They are Conservatives to be put on the Supreme Court that don’t have all of these blemishes on their record these questions of their character there are better candidates. They need to find one. This one is the POTUS pic because he thinks the POTUS should be above the law',\n",
       "  ' Uhhh no.  Sadam gassed the shit out of the Kurds (sp?)',\n",
       "  ' Does Eric even know where he is?',\n",
       "  '. just picked up my  post on gun confiscation.  This is the history of gun control your high school and college history classes will NEVER tell you about.   URL',\n",
       "  '  Shove it The View you are the most despicable show on tv and like everyone always says “who watches these bimbos”!',\n",
       "  ' Do you happen to know what study Qu’ran she is using in the film?',\n",
       "  '    Antifa are brown shirts. You... just special.',\n",
       "  ' Take a stand against all violence!  Talk to your ANTIFA friends.',\n",
       "  ' Hell ya she is',\n",
       "  ' They cite Jones being banned for violating Twitter\\'s ToS. There are blue checkmarks spewing the same, if not worse, kind of shit. If you are going to play the anyone can get banned\" card. Shouldn\\'t these people also receive bans and suspensions? #VerifiedHate\"',\n",
       "  '    Just give up. You are desperate to spread hate &amp; the more you do the more we send love to the victim. You have lost your humanity. Stop and think what you are trying to achieve because you look like you want to start a war on innocent people.',\n",
       "  ' A MASSIVE TOOL',\n",
       "  ' said on  he doesn’t think we should be selling AR15’s but Liberals deny it and say he is pro gun! The AR15 is not an assault weapon it is a sporting rifle. #KeepTexasRed vote ',\n",
       "  \" Hey Pauly .... when the rest of the countries catch up with USA's environmental policies .. let us know !!\",\n",
       "  ' Keep #MAGA 🇺🇸🇺🇸🇺🇸🇺🇸 URL',\n",
       "  ' same shit lmaaaaoooo. i was just making sure!',\n",
       "  '     Haha 😂 Trump is still winning. It’s fun to watch the liberals literally go insane. URL')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![embedding](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/b4efbefa47672174394a8b6a27d4e7bc193bc224/assets/sentiment8.png)\n",
    "\n",
    "_image from bentrevett_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "k85Rbx-3ZLq9"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BoWClassifierWithEmbedding(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, embedding_dim):\n",
    "        super(BoWClassifierWithEmbedding, self).__init__()\n",
    "\n",
    "        # We define the embedding layer here\n",
    "        # It will convert a list of ids: [1, 50, 64, 2006]\n",
    "        # Into a list of vectors, one for each word\n",
    "        # The embedding layer will learn the vectors from the contexts\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=3001)\n",
    "        # We could also load precomputed embeddings, e.g. GloVe, in some cases we don't want to train the embedding layer\n",
    "        # In this case we enable the training\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        self.linear = nn.Linear(embedding_dim, num_labels)\n",
    "\n",
    "    def forward(self, text, sequence_lens):\n",
    "        # First we create the embedded vectors\n",
    "        embedded = self.embedding(text)\n",
    "        # We need a pooling to convert a list of embedded words to a sentence vector\n",
    "        # We could have chosen different pooling, e.g. min, max, average..\n",
    "        # With LSTM we also do a pooling, just smarter\n",
    "        pooled = F.max_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n",
    "        return F.log_softmax(self.linear(pooled), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of the LSTM layer..\n",
    "\n",
    "![lstm](https://i.stack.imgur.com/SjnTl.png)\n",
    "\n",
    "_image from stackoverflow_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=3001)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        # Documentation: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "        # Dropout to overcome overfitting\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, text, sequence_lens):\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # To ensure LSTM doesn't learn gradients for the id of the padding symbol\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, sequence_lens, enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        packed_outputs, (h, c) = self.lstm(packed)\n",
    "        # extract LSTM outputs (not used here)\n",
    "        lstm_outputs, lens = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outputs, batch_first=True\n",
    "        )\n",
    "\n",
    "        # We use the last hidden vector from LSTM\n",
    "        y = self.linear(h[-1])\n",
    "        log_probs = F.log_softmax(y, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "9QgKkr4bZSPG"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = VOCAB_SIZE + 2\n",
    "OUTPUT_DIM = 2\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 20\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# model = BoWClassifierWithEmbedding(OUTPUT_DIM, INPUT_DIM, EMBEDDING_DIM)\n",
    "model = LSTMClassifier(OUTPUT_DIM, INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "oVrF6CgmZUeg"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "bHCHriijZU9F",
    "outputId": "75570e6e-4069-48bd-d20d-d2c4529b5a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.640 | Train Prec: 9.36% | Train Rec: 44.51% | Train Fscore: 13.57%\n",
      "\t Val. Loss: 0.628 |  Val Prec: 5.59% | Val Rec: 38.91% | Val Fscore: 9.17%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.616 | Train Prec: 8.26% | Train Rec: 62.91% | Train Fscore: 14.09%\n",
      "\t Val. Loss: 0.619 |  Val Prec: 13.26% | Val Rec: 57.58% | Val Fscore: 20.16%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.577 | Train Prec: 24.45% | Train Rec: 74.80% | Train Fscore: 35.52%\n",
      "\t Val. Loss: 0.600 |  Val Prec: 27.24% | Val Rec: 60.82% | Val Fscore: 36.16%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.513 | Train Prec: 47.20% | Train Rec: 74.12% | Train Fscore: 56.60%\n",
      "\t Val. Loss: 0.588 |  Val Prec: 38.15% | Val Rec: 59.75% | Val Fscore: 45.68%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.446 | Train Prec: 58.27% | Train Rec: 77.58% | Train Fscore: 65.68%\n",
      "\t Val. Loss: 0.593 |  Val Prec: 48.63% | Val Rec: 57.97% | Val Fscore: 52.06%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.383 | Train Prec: 67.19% | Train Rec: 82.08% | Train Fscore: 73.43%\n",
      "\t Val. Loss: 0.617 |  Val Prec: 45.37% | Val Rec: 59.41% | Val Fscore: 50.80%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.330 | Train Prec: 73.07% | Train Rec: 85.44% | Train Fscore: 78.31%\n",
      "\t Val. Loss: 0.651 |  Val Prec: 48.49% | Val Rec: 60.02% | Val Fscore: 52.73%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.284 | Train Prec: 78.42% | Train Rec: 88.54% | Train Fscore: 82.87%\n",
      "\t Val. Loss: 0.696 |  Val Prec: 47.21% | Val Rec: 58.49% | Val Fscore: 51.60%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.237 | Train Prec: 82.31% | Train Rec: 90.47% | Train Fscore: 85.83%\n",
      "\t Val. Loss: 0.759 |  Val Prec: 54.29% | Val Rec: 54.86% | Val Fscore: 53.89%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.202 | Train Prec: 85.40% | Train Rec: 92.82% | Train Fscore: 88.70%\n",
      "\t Val. Loss: 0.809 |  Val Prec: 50.28% | Val Rec: 56.64% | Val Fscore: 52.58%\n",
      "Epoch: 11 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.169 | Train Prec: 88.52% | Train Rec: 94.04% | Train Fscore: 90.99%\n",
      "\t Val. Loss: 0.902 |  Val Prec: 50.86% | Val Rec: 58.73% | Val Fscore: 53.98%\n",
      "Epoch: 12 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.142 | Train Prec: 90.76% | Train Rec: 95.21% | Train Fscore: 92.74%\n",
      "\t Val. Loss: 1.000 |  Val Prec: 50.14% | Val Rec: 56.86% | Val Fscore: 52.68%\n",
      "Epoch: 13 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.121 | Train Prec: 92.35% | Train Rec: 96.25% | Train Fscore: 94.11%\n",
      "\t Val. Loss: 1.054 |  Val Prec: 48.19% | Val Rec: 54.74% | Val Fscore: 50.72%\n",
      "Epoch: 14 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.102 | Train Prec: 93.80% | Train Rec: 97.29% | Train Fscore: 95.42%\n",
      "\t Val. Loss: 1.162 |  Val Prec: 51.60% | Val Rec: 57.16% | Val Fscore: 53.51%\n",
      "Epoch: 15 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.085 | Train Prec: 94.41% | Train Rec: 97.65% | Train Fscore: 95.90%\n",
      "\t Val. Loss: 1.253 |  Val Prec: 50.58% | Val Rec: 55.30% | Val Fscore: 52.10%\n"
     ]
    }
   ],
   "source": [
    "training_loop(epoch_number=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLx6p6f2Y69C"
   },
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To completely understand the transformers architecture look at this lecture held by Judit Acs (on the course of Introduction to Python and Natural Language Technologies in BME): \n",
    "- https://github.com/bmeaut/python_nlp_2021_spring/blob/main/lectures/09_Transformers_BERT.ipynb\n",
    "\n",
    "Here I will only include and present the necessary details _from the lecture_ about transformers and BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with recurrent neural networks:\n",
    "\n",
    "Recall that we used recurrent neural cells, specifically LSTMs to encode a list of vectors into a sentence vector.\n",
    "\n",
    "- Problem 1. No parallelism\n",
    "\n",
    "        - LSTMs are recurrent, they rely on their left and right history, so the symbols need to be processed in order -> no parallelism.\n",
    "\n",
    "- Problem 2. Long-range dependencies\n",
    "\n",
    "        - Long-range dependencies are not infrequent in NLP.\n",
    "\n",
    "        - \"The people/person who called and wanted to rent your house when you go away next year are/is from California\" -- Miller & Chomsky 1963\n",
    "\n",
    "        - LSTMs have a problem capturing these because there are too many backpropagation steps between the symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduced in [Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) by Vaswani et al., 2017\n",
    "\n",
    "Transformers solve Problem 1 by relying purely on attention instead of recurrence.\n",
    "\n",
    "Not having recurrent connections means that sequence position no longer matters.\n",
    "\n",
    "Recurrence is replaced by self attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformers are available in the __transformers__ Python package: https://github.com/huggingface/transformers.\n",
    "- There are thousands of pretrained transformers models in different languages and with different architectures. \n",
    "- With the huggingface package there is a unified interface to download and use all the models. Browse https://huggingface.co/models for more!\n",
    "- There is also a great blog post to understand the architecture of transformers: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "[BERT](https://www.aclweb.org/anthology/N19-1423/): Pre-training of Deep Bidirectional Transformers for Language Understanding by Devlin et al. 2018, 17500 citations\n",
    "\n",
    "[BERTology](https://huggingface.co/transformers/bertology.html) is the nickname for the growing amount of BERT-related research.\n",
    "\n",
    "BERT trains a transformer model on two tasks:\n",
    "\n",
    "- Masked language model:\n",
    "\n",
    "    - 15% of the tokenswordpieces are selected at the beginning.\n",
    "    - 80% of those are replaced with [MASK],\n",
    "    - 10% are replaced with a random token,\n",
    "    - 10% are kept intact.\n",
    "\n",
    "- Next sentence prediction:\n",
    "    - Are sentences A and B consecutive sentences?\n",
    "    - Generate 50-50%.\n",
    "    - Binary classification task.\n",
    "    \n",
    "\n",
    "### Training, Finetuning BERT\n",
    "\n",
    "- BERT models are (masked-)language models that were usually trained on large corporas.\n",
    "\n",
    "- e.g. BERT base model was trained on BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia.\n",
    "\n",
    "#### Finetuning\n",
    "\n",
    "- Get a trained BERT model.\n",
    "- Add a small classification layer on top (typically a 2-layer MLP).\n",
    "- Train BERT along with the classification layer on an annotated dataset.\n",
    "- Much smaller than the data BERT was trained on\n",
    "- Another option: freeze BERT and train the classification layer only.\n",
    "    - Easier training regime.\n",
    "    - Smaller memory footprint.\n",
    "    - Worse performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg\" alt=\"finetune\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece tokenizer\n",
    "- BERT has its own tokenizer\n",
    "- All inputs must be tokenized with BERT \n",
    "- You don't need to remove stopwords, lemmatize, preprocess the input for BERT\n",
    "\n",
    "- It is a middle ground between word and character tokenization.\n",
    "\n",
    "- Static vocabulary:\n",
    "    - Special tokens: [CLS], [SEP], [MASK], [UNK]\n",
    "    - It tokenizes everything, falling back to characters and [UNK] if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "CkBZHPfqPwZ-"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467,
     "referenced_widgets": [
      "096810167d924a7da4ad7c4538c50a04",
      "0af1cd00129d49d4a31217aa51dc6492",
      "ae22a36a43f9401d857fc676be1278f5",
      "07db6b1c39654a22900ed9b917e34d25",
      "a7010a8925694b04821fe56296bfbf2f",
      "dfe4c176f62f4d72a3eed57e1b1fc87f",
      "c9ca64235fde44c89bb911d352610442",
      "36726f26d1f143fd9474d5dce2619792",
      "8d4f4440a6a04ed6bc5a8eb4db590298",
      "24819d782a6140608a6c588afbc9b702",
      "1d6df86820ec45d6994ac74159ee9228",
      "2bb76615fa704275a081f03ea3f8cf5e",
      "a333b5da45ef4fb1932632601ad4f6ad",
      "206f338165c1467ea0bcf55d9d471af7",
      "5ae77de2d5d94c2788ea9080584d713d",
      "580b5a5e7090490db50460666bac7c81",
      "0b282fb195f24780a0519b91c4f1b344",
      "99d43a7fb97c4ada9ea664b2b171f732",
      "55e8e84304c74f4797c783a335a0f3e3",
      "8c607ade94ff4d6eb5a9db3609d83f92",
      "b38a23c10fbd4718b4b035969b1c5e63",
      "5f0d6531228a4157bddadc813d5534eb",
      "d0cf263890ab4d12a1c4926081494f7a",
      "9b5d87c13f3b41d0ab97b373ef386bdb",
      "d99fe5b061ea487db430d81effd5d26d",
      "e2d0984c7c1d4b0b94730d79d6af05fb",
      "a1e5eb0cf27e496ba2bca21d2c58f110",
      "53bab86ab477490091c0b93987200f7b",
      "0cca3ccf52bc49d9852f05004ed055da",
      "235d878939c344ab875231cb878fca53",
      "5feb0980894e4dd9a7598a1f25b54a0c",
      "240cae4992ce4038a6e919cad26ed18c",
      "74f840feaa5e496495f904d01df4fb07",
      "3c1ebe4b499847c39be7e3c3d9e8633f",
      "7bdcda40469641c6bc342ee12be14cc9",
      "2c26c0d144f240569046450fbec8f22c",
      "77892da04de543e9874c6895d67c2b63",
      "1edb014f1bd2456ea3e32c3d88fd978d",
      "023713fa09c24b2f8b3af722b25c79fa",
      "e6f37ec804244e499af12cedc804e130",
      "d60c5031673c41de90ab6f9db55641c2",
      "dde18a6e6bd74e88942a953792f4f37d",
      "94583cf009f54a4cba00bb5893e2d2c3",
      "c98ae3150e2747fea3027f7a79deb4be",
      "2fb629ac07334b768b12060e75972976",
      "df7172cfa535480690a6c9a402460f4e",
      "5b361c6a04024f92a8f6b5d39c921826",
      "22c034898e3f4bffa2e2b3e72dfd5125",
      "fc8e95563eef4cd3bd2741cf9545d1a7",
      "e5bdf71518aa4098885d923927eeb1d8",
      "e17dc125b2414d09b29e5ca69a6f9231",
      "7345e744a8d1426fb5cab6f8cd325a90",
      "aa7b97e741394bc08e108d2187750cb3",
      "b60f66d91f4a4b409a69f37c40c70dd1",
      "2e0991876c794ff9bc1550f5dcabac73"
     ]
    },
    "id": "__eq23ZTP21d",
    "outputId": "76632030-18a2-4ef7-98ed-ca336363e0cf"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'shi',\n",
       " '##ht',\n",
       " '##zu',\n",
       " \"'\",\n",
       " 's',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mas',\n",
       " '##za',\n",
       " '##t',\n",
       " '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(tokenizer))\n",
    "print(len(tokenizer.get_vocab()))\n",
    "\n",
    "tokenizer.tokenize(\"My shihtzu's name is Maszat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2045, 2024, 2304, 8870, 1998, 2304, 6077, 1012, 102, 2178, 6251, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"There are black cats and black dogs.\", \"Another sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a BertForSequenceClassification model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BertForSequenceClassification__ is a helper class to train transformer-based BERT models. It puts a classification layer on top of a pretrained model.\n",
    "\n",
    "Read more in the documentation: https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "IOOymZT0ZNs9"
   },
   "outputs": [],
   "source": [
    "# We only want to finetune the classification layer on top of BERT\n",
    "for p in model.base_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight (2, 768)\n",
      "bert.embeddings.LayerNorm.weight (768,)\n",
      "bert.embeddings.LayerNorm.bias (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight (768, 768)\n",
      "bert.pooler.dense.bias (768,)\n",
      "classifier.weight (2, 768)\n",
      "classifier.bias (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print(f\"The BERT model has {len(params)} different named parameters.\")\n",
    "\n",
    "print(\"==== Embedding Layer ====\\n\")\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(f\"{p[0]} {str(tuple(p[1].size()))}\")\n",
    "\n",
    "print(\"\\n==== First Transformer ====\\n\")\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(f\"{p[0]} {str(tuple(p[1].size()))}\")\n",
    "\n",
    "print(\"\\n==== Output Layer ====\\n\")\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(f\"{p[0]} {str(tuple(p[1].size()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data_loader, val_data_loader = prepare_dataloader_with_padding(\n",
    "    tr_data, val_data, word_to_ix\n",
    ")\n",
    "\n",
    "train_iterator, valid_iterator = create_dataloader_iterators_with_padding(\n",
    "    tr_data_loader, val_data_loader, BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaamko/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.640 | Train Prec: 5.10% | Train Rec: 12.49% | Train Fscore: 5.08%\n",
      "\t Val. Loss: 0.661 |  Val Prec: 0.00% | Val Rec: 0.00% | Val Fscore: 0.00%\n",
      "Epoch: 02 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.614 | Train Prec: 7.89% | Train Rec: 41.22% | Train Fscore: 12.05%\n",
      "\t Val. Loss: 0.597 |  Val Prec: 2.25% | Val Rec: 21.98% | Val Fscore: 3.96%\n",
      "Epoch: 03 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.597 | Train Prec: 13.21% | Train Rec: 56.39% | Train Fscore: 19.26%\n",
      "\t Val. Loss: 0.580 |  Val Prec: 16.67% | Val Rec: 70.27% | Val Fscore: 26.07%\n",
      "Epoch: 04 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.590 | Train Prec: 20.10% | Train Rec: 58.86% | Train Fscore: 26.62%\n",
      "\t Val. Loss: 0.572 |  Val Prec: 10.38% | Val Rec: 66.87% | Val Fscore: 17.28%\n",
      "Epoch: 05 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.581 | Train Prec: 22.31% | Train Rec: 60.05% | Train Fscore: 30.59%\n",
      "\t Val. Loss: 0.566 |  Val Prec: 13.55% | Val Rec: 69.60% | Val Fscore: 21.88%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_prec = 0\n",
    "    train_epoch_recall = 0\n",
    "    train_epoch_fscore = 0\n",
    "    model.train()\n",
    "\n",
    "    # We use our own iterator but now use the raw texts instead of the ID tokens\n",
    "    for train_batch in train_iterator:\n",
    "        labels = train_batch[1]\n",
    "        texts = train_batch[3]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # We use BERT's own tokenizer on raw texts\n",
    "        # Check the documentation: https://huggingface.co/transformers/main_classes/tokenizer.html\n",
    "        encoded = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # BERT converts texts into IDs of its own vocabulary\n",
    "        input_ids = encoded[\"input_ids\"].to(device)\n",
    "        # Mask to avoid performing attention on padding token indices.\n",
    "        attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Run the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        predictions = outputs[1]\n",
    "        prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "        train_epoch_prec += prec.item()\n",
    "        train_epoch_recall += recall.item()\n",
    "        train_epoch_fscore += fscore.item()\n",
    "\n",
    "    train_loss = train_epoch_loss / len(train_iterator)\n",
    "    train_prec = train_epoch_prec / len(train_iterator)\n",
    "    train_rec = train_epoch_recall / len(train_iterator)\n",
    "    train_fscore = train_epoch_fscore / len(train_iterator)\n",
    "\n",
    "    # And validate your model on the validation set\n",
    "    valid_epoch_loss = 0\n",
    "    valid_epoch_prec = 0\n",
    "    valid_epoch_recall = 0\n",
    "    valid_epoch_fscore = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for valid_batch in valid_iterator:\n",
    "            labels = valid_batch[1]\n",
    "            texts = valid_batch[3]\n",
    "\n",
    "            encoded = tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            predictions = outputs[1]\n",
    "            prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "\n",
    "            # We add batch-wise loss to the epoch-wise loss\n",
    "            valid_epoch_loss += loss.item()\n",
    "            valid_epoch_prec += prec.item()\n",
    "            valid_epoch_recall += recall.item()\n",
    "            valid_epoch_fscore += fscore.item()\n",
    "\n",
    "    valid_loss = valid_epoch_loss / len(valid_iterator)\n",
    "    valid_prec = valid_epoch_prec / len(valid_iterator)\n",
    "    valid_rec = valid_epoch_recall / len(valid_iterator)\n",
    "    valid_fscore = valid_epoch_fscore / len(valid_iterator)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(\n",
    "        f\"\\tTrain Loss: {train_loss:.3f} | Train Prec: {train_prec*100:.2f}% | Train Rec: {train_rec*100:.2f}% | Train Fscore: {train_fscore*100:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\t Val. Loss: {valid_loss:.3f} |  Val Prec: {valid_prec*100:.2f}% | Val Rec: {valid_rec*100:.2f}% | Val Fscore: {valid_fscore*100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Deep learning - practical lesson",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "023713fa09c24b2f8b3af722b25c79fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07db6b1c39654a22900ed9b917e34d25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d4f4440a6a04ed6bc5a8eb4db590298",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36726f26d1f143fd9474d5dce2619792",
      "value": 231508
     }
    },
    "096810167d924a7da4ad7c4538c50a04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae22a36a43f9401d857fc676be1278f5",
       "IPY_MODEL_07db6b1c39654a22900ed9b917e34d25",
       "IPY_MODEL_a7010a8925694b04821fe56296bfbf2f"
      ],
      "layout": "IPY_MODEL_0af1cd00129d49d4a31217aa51dc6492"
     }
    },
    "0af1cd00129d49d4a31217aa51dc6492": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b282fb195f24780a0519b91c4f1b344": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0cca3ccf52bc49d9852f05004ed055da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d6df86820ec45d6994ac74159ee9228": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1edb014f1bd2456ea3e32c3d88fd978d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c98ae3150e2747fea3027f7a79deb4be",
      "placeholder": "​",
      "style": "IPY_MODEL_94583cf009f54a4cba00bb5893e2d2c3",
      "value": " 570/570 [00:00&lt;00:00, 13.2kB/s]"
     }
    },
    "206f338165c1467ea0bcf55d9d471af7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99d43a7fb97c4ada9ea664b2b171f732",
      "placeholder": "​",
      "style": "IPY_MODEL_0b282fb195f24780a0519b91c4f1b344",
      "value": "Downloading: 100%"
     }
    },
    "22c034898e3f4bffa2e2b3e72dfd5125": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa7b97e741394bc08e108d2187750cb3",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7345e744a8d1426fb5cab6f8cd325a90",
      "value": 440473133
     }
    },
    "235d878939c344ab875231cb878fca53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "240cae4992ce4038a6e919cad26ed18c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24819d782a6140608a6c588afbc9b702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bb76615fa704275a081f03ea3f8cf5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_206f338165c1467ea0bcf55d9d471af7",
       "IPY_MODEL_5ae77de2d5d94c2788ea9080584d713d",
       "IPY_MODEL_580b5a5e7090490db50460666bac7c81"
      ],
      "layout": "IPY_MODEL_a333b5da45ef4fb1932632601ad4f6ad"
     }
    },
    "2c26c0d144f240569046450fbec8f22c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6f37ec804244e499af12cedc804e130",
      "placeholder": "​",
      "style": "IPY_MODEL_023713fa09c24b2f8b3af722b25c79fa",
      "value": "Downloading: 100%"
     }
    },
    "2e0991876c794ff9bc1550f5dcabac73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fb629ac07334b768b12060e75972976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b361c6a04024f92a8f6b5d39c921826",
       "IPY_MODEL_22c034898e3f4bffa2e2b3e72dfd5125",
       "IPY_MODEL_fc8e95563eef4cd3bd2741cf9545d1a7"
      ],
      "layout": "IPY_MODEL_df7172cfa535480690a6c9a402460f4e"
     }
    },
    "36726f26d1f143fd9474d5dce2619792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c1ebe4b499847c39be7e3c3d9e8633f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c26c0d144f240569046450fbec8f22c",
       "IPY_MODEL_77892da04de543e9874c6895d67c2b63",
       "IPY_MODEL_1edb014f1bd2456ea3e32c3d88fd978d"
      ],
      "layout": "IPY_MODEL_7bdcda40469641c6bc342ee12be14cc9"
     }
    },
    "53bab86ab477490091c0b93987200f7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55e8e84304c74f4797c783a335a0f3e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "580b5a5e7090490db50460666bac7c81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f0d6531228a4157bddadc813d5534eb",
      "placeholder": "​",
      "style": "IPY_MODEL_b38a23c10fbd4718b4b035969b1c5e63",
      "value": " 28.0/28.0 [00:00&lt;00:00, 581B/s]"
     }
    },
    "5ae77de2d5d94c2788ea9080584d713d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c607ade94ff4d6eb5a9db3609d83f92",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55e8e84304c74f4797c783a335a0f3e3",
      "value": 28
     }
    },
    "5b361c6a04024f92a8f6b5d39c921826": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e17dc125b2414d09b29e5ca69a6f9231",
      "placeholder": "​",
      "style": "IPY_MODEL_e5bdf71518aa4098885d923927eeb1d8",
      "value": "Downloading: 100%"
     }
    },
    "5f0d6531228a4157bddadc813d5534eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5feb0980894e4dd9a7598a1f25b54a0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7345e744a8d1426fb5cab6f8cd325a90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74f840feaa5e496495f904d01df4fb07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77892da04de543e9874c6895d67c2b63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dde18a6e6bd74e88942a953792f4f37d",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d60c5031673c41de90ab6f9db55641c2",
      "value": 570
     }
    },
    "7bdcda40469641c6bc342ee12be14cc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c607ade94ff4d6eb5a9db3609d83f92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d4f4440a6a04ed6bc5a8eb4db590298": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94583cf009f54a4cba00bb5893e2d2c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99d43a7fb97c4ada9ea664b2b171f732": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b5d87c13f3b41d0ab97b373ef386bdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1e5eb0cf27e496ba2bca21d2c58f110": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74f840feaa5e496495f904d01df4fb07",
      "placeholder": "​",
      "style": "IPY_MODEL_240cae4992ce4038a6e919cad26ed18c",
      "value": " 455k/455k [00:00&lt;00:00, 1.93MB/s]"
     }
    },
    "a333b5da45ef4fb1932632601ad4f6ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7010a8925694b04821fe56296bfbf2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d6df86820ec45d6994ac74159ee9228",
      "placeholder": "​",
      "style": "IPY_MODEL_24819d782a6140608a6c588afbc9b702",
      "value": " 226k/226k [00:00&lt;00:00, 814kB/s]"
     }
    },
    "aa7b97e741394bc08e108d2187750cb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae22a36a43f9401d857fc676be1278f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9ca64235fde44c89bb911d352610442",
      "placeholder": "​",
      "style": "IPY_MODEL_dfe4c176f62f4d72a3eed57e1b1fc87f",
      "value": "Downloading: 100%"
     }
    },
    "b38a23c10fbd4718b4b035969b1c5e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b60f66d91f4a4b409a69f37c40c70dd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c98ae3150e2747fea3027f7a79deb4be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9ca64235fde44c89bb911d352610442": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0cf263890ab4d12a1c4926081494f7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d99fe5b061ea487db430d81effd5d26d",
       "IPY_MODEL_e2d0984c7c1d4b0b94730d79d6af05fb",
       "IPY_MODEL_a1e5eb0cf27e496ba2bca21d2c58f110"
      ],
      "layout": "IPY_MODEL_9b5d87c13f3b41d0ab97b373ef386bdb"
     }
    },
    "d60c5031673c41de90ab6f9db55641c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d99fe5b061ea487db430d81effd5d26d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cca3ccf52bc49d9852f05004ed055da",
      "placeholder": "​",
      "style": "IPY_MODEL_53bab86ab477490091c0b93987200f7b",
      "value": "Downloading: 100%"
     }
    },
    "dde18a6e6bd74e88942a953792f4f37d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df7172cfa535480690a6c9a402460f4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfe4c176f62f4d72a3eed57e1b1fc87f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e17dc125b2414d09b29e5ca69a6f9231": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2d0984c7c1d4b0b94730d79d6af05fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5feb0980894e4dd9a7598a1f25b54a0c",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_235d878939c344ab875231cb878fca53",
      "value": 466062
     }
    },
    "e5bdf71518aa4098885d923927eeb1d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6f37ec804244e499af12cedc804e130": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc8e95563eef4cd3bd2741cf9545d1a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e0991876c794ff9bc1550f5dcabac73",
      "placeholder": "​",
      "style": "IPY_MODEL_b60f66d91f4a4b409a69f37c40c70dd1",
      "value": " 420M/420M [00:16&lt;00:00, 25.9MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
